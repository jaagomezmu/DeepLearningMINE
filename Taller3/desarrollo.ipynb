{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib64/python3.12/site-packages (24.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 01:28:24.006858: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-29 01:28:24.010275: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-29 01:28:24.020355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730183304.037392   59709 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730183304.042510   59709 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-29 01:28:24.059316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import contractions\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LSTM,\n",
    "    TextVectorization,\n",
    "    Embedding,\n",
    "    Bidirectional,\n",
    "    GRU\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import numpy as np\n",
    "import num2words\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from wordcloud import WordCloud\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "% matplotlib inline\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes\n",
    "DATA_PATH = Path(\"tweet-gpt/tweet_gpt.csv\")\n",
    "MY_SEED = 42\n",
    "SENTIMENT_FIELD = \"sentiment\"\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "TEXT_FIELD = \"Tweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones\n",
    "def remove_stopwords(words):\n",
    "  for word in STOPWORDS:\n",
    "    token = ' ' + word + ' '\n",
    "    words = re.sub(token, ' ', words)\n",
    "  return words\n",
    "\n",
    "def stem_and_lemmatize(words, apply_stem=True, apply_lemmatize=True):\n",
    "    if apply_lemmatize:\n",
    "        lemmatized_words = \" \".join([WordNetLemmatizer().lemmatize(word) for word in words.split()])\n",
    "    else:\n",
    "        lemmatized_words = words\n",
    "\n",
    "    if apply_stem:\n",
    "        stemmed_words = \" \".join([PorterStemmer().stem(word) for word in lemmatized_words.split()])\n",
    "    else:\n",
    "        stemmed_words = lemmatized_words\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def preproccesing(words, apply_stem=True, apply_lemmatize=True):\n",
    "    words = words.lower()\n",
    "    words = re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0))), words)\n",
    "    words = re.sub(r\"[\\\"(),¡!¿?:;'>]\", \"\", words)\n",
    "    words = unicodedata.normalize('NFKD', words).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    words = remove_stopwords(words)\n",
    "    words = stem_and_lemmatize(words,apply_stem, apply_lemmatize)\n",
    "    words = \" \".join([word for word in words.split() if len(word) > 1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "if not DATA_PATH.exists:\n",
    "    api.dataset_download_files(\n",
    "        'evilspirit05/tweet-gpt',\n",
    "        path='tweet-gpt',\n",
    "        unzip=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH)\n",
    "display(data.head(3))\n",
    "print()\n",
    "print(data[SENTIMENT_FIELD].value_counts())\n",
    "print()\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data[SENTIMENT_FIELD], random_state=MY_SEED, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.2, stratify=train[SENTIMENT_FIELD], random_state=MY_SEED, shuffle=True)\n",
    "print(\"Tamaño de datos de entrenamiento:\", train.shape)\n",
    "print(\"Tamaño de datos de validación:\", val.shape)\n",
    "print(\"Tamaño de datos de prueba:\", test.shape)\n",
    "print()\n",
    "X_train, X_test, X_val= train[TEXT_FIELD], test[TEXT_FIELD], val[TEXT_FIELD]\n",
    "y_train, y_test, y_val= train[SENTIMENT_FIELD], test[SENTIMENT_FIELD], val[SENTIMENT_FIELD]\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
    "print()\n",
    "display(X_train.head(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentiments = y_train.unique()\n",
    "print(unique_sentiments, \"\\n\")\n",
    "\n",
    "for sentiment in unique_sentiments:\n",
    "  row = data[data[SENTIMENT_FIELD] == sentiment].iloc[0]\n",
    "  print(f\"Sentiment:\", sentiment)\n",
    "  print(f\"Sentence:\", row[TEXT_FIELD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar contractions.fix and preprocessing en los conjuntos\n",
    "data.loc[X_train.index, 'Tweet'] = X_train.apply(contractions.fix).apply(preproccesing)\n",
    "data.loc[X_val.index, 'Tweet'] = X_val.apply(contractions.fix).apply(preproccesing)\n",
    "data.loc[X_test.index, 'Tweet'] = X_test.apply(contractions.fix).apply(preproccesing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply contractions.fix and preprocessing to the training, validation, and test sets\n",
    "X_train = X_train.apply(contractions.fix).apply(preproccesing)\n",
    "X_val = X_val.apply(contractions.fix).apply(preproccesing)\n",
    "X_test = X_test.apply(contractions.fix).apply(preproccesing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar un reporte exploratorio automático\n",
    "profile = ProfileReport(data[['Tweet']], title='Reporte EDA', html={'style':{'full_width':True}})\n",
    "profile.to_notebook_iframe()\n",
    "text_data = ' '.join(data['Tweet'].astype(str))\n",
    "print(f\"Tamaño del texto para WordCloud: {len(text_data)}\")\n",
    "wordcloud = WordCloud(width=1600, height=800).generate(text_data)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from X_train\n",
    "X_train_clean = X_train.drop_duplicates()\n",
    "# Now, filter y_train using the same indices that were kept in X_train_clean\n",
    "y_train_clean = y_train[X_train.index.isin(X_train_clean.index)]\n",
    "# Verify the new shapes after removing duplicates\n",
    "print(f\"Shape of X_train after removing duplicates: {X_train_clean.shape}\")\n",
    "print(f\"Shape of y_train after removing duplicates: {y_train_clean.shape}\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the encoder only on y_train and transform y_train\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# Transform y_val and y_test using the same label encoder\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "# Get the original class labels\n",
    "unique_labels = label_encoder.classes_\n",
    "# Print the mapping between numeric values and original labels\n",
    "for valor_numerico, etiqueta_original in enumerate(unique_labels):\n",
    "    print(f'Valor numérico: {valor_numerico}, Etiqueta original: {etiqueta_original}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la capa de TextVectorization para tokenizar el texto\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,  # No aplicar normalización adicional (puedes ajustar según lo que necesites)\n",
    "    split=\"whitespace\",  # Dividir el texto por espacios en blanco\n",
    "    max_tokens=None,  # Puedes limitar el número máximo de tokens si es necesario\n",
    "    output_mode='int',  # Convertir el texto en secuencias de enteros\n",
    "    output_sequence_length=None  # Longitud de la secuencia (puedes ajustar si necesitas padding)\n",
    ")\n",
    "\n",
    "# Adaptar la capa con los datos de entrenamiento (X_train)\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la capa vectorize_layer sobre los conjuntos de entrenamiento, validación y prueba\n",
    "X_train_vectorized = vectorize_layer(X_train)\n",
    "X_val_vectorized = vectorize_layer(X_val)\n",
    "X_test_vectorized = vectorize_layer(X_test)\n",
    "\n",
    "row_index = 8010\n",
    "print(\"Texto original en X_train:\", X_train.iloc[row_index], end=\"\\n\")\n",
    "print(\"Texto tokenizado en X_train_vectorized:\", X_train_vectorized[row_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulario:\", vectorize_layer.get_vocabulary()[:100])\n",
    "print(\"Configuración:\", vectorize_layer.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(text.split()) for text in X_train]\n",
    "plt.hist(sequence_lengths, bins=50)\n",
    "plt.xlabel('Longitud de Secuencia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=4),\n",
    "    # tf.keras.callbacks.TensorBoard(log_dir='/content/logs'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(10000, 300, name=\"Capa_Embedding\"))\n",
    "\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1024, activation='relu', name='Capa_Oculta'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3, activation='softmax', name='Capa_Salida'))  # 3 para 3 clases\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      X_train_tf, y_train,\n",
    "      validation_data = (X_val_tf, y_val),\n",
    "      epochs=20,\n",
    "      callbacks=my_callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
